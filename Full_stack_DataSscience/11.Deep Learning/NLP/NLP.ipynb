{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Introduction and Text Preprocessing"
      ],
      "metadata": {
        "id": "HMNgkBAn8MQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`1. What is the primary goal of Natural Language Processing (NLP)?`\n",
        "\n",
        "The primary goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is meaningful and useful. It involves tasks like text classification, sentiment analysis, machine translation, speech recognition, and conversational AI, bridging the gap between human communication and machine understanding.\n",
        "\n",
        "`2. What does \"tokenization\" refer to in text processing?`\n",
        "\n",
        "Tokenization is the process of breaking down a text into smaller units, such as words or sentences.\n",
        "\n",
        "- Word Tokenization: Splitting a sentence into individual words (e.g., \"Natural Language Processing\" → [\"Natural\", \"Language\", \"Processing\"]).\n",
        "- Sentence Tokenization: Splitting a paragraph into individual sentences (e.g., \"NLP is fascinating. It has many applications.\" → [\"NLP is fascinating.\", \"It has many applications.\"]).\n",
        "\n",
        "`3. What is the difference between lemmatization and stemming?`\n",
        "\n",
        "- Lemmatization: Reduces a word to its base or dictionary form (lemma) while preserving meaning and context. For example, \"running\" → \"run\", \"better\" → \"good\".\n",
        "- Stemming: Reduces a word to its root form by removing suffixes, often without considering the meaning. For example, \"running\" → \"run\", \"happiness\" → \"happi\".\n",
        "- Key Difference: Lemmatization is more accurate and context-aware, while stemming is faster but may produce words that are not linguistically valid.\n",
        "\n",
        "`4. What is the role of regular expressions (regex) in text processing?`\n",
        "\n",
        "Regular expressions (regex) are used for pattern matching and text manipulation. In NLP, they help:\n",
        "\n",
        "Extract specific patterns like email addresses, phone numbers, or dates.\n",
        "Perform text cleaning, such as removing special characters or extra spaces.\n",
        "Identify and replace certain patterns in text efficiently.\n",
        "\n",
        "`5. What is Word2Vec and how does it represent words in a vector space?`\n",
        "\n",
        "Word2Vec is a word embedding model that represents words as continuous dense vectors in a high-dimensional space. It uses two main approaches:\n",
        "\n",
        "- CBOW (Continuous Bag of Words): Predicts a word given its surrounding context.\n",
        "- Skip-gram: Predicts surrounding context words given a target word.\n",
        "Word2Vec captures semantic relationships between words, so similar words are located closer together in the vector space (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\").\n",
        "\n",
        "`6. How does frequency distribution help in text analysis?`\n",
        "\n",
        "Frequency distribution counts the occurrence of words or tokens in a text. It helps in:\n",
        "\n",
        "- Identifying commonly used words (e.g., frequent keywords in a document).\n",
        "- Visualizing text characteristics (e.g., using word clouds).\n",
        "- Highlighting stopwords that might need removal.\n",
        "- Analyzing patterns and trends in text data for further insights.\n",
        "\n",
        "`7. Why is text normalization important in NLP?`\n",
        "\n",
        "Text normalization standardizes text to ensure uniformity and reduce variability caused by different formats, styles, or errors. It includes:\n",
        "\n",
        "- Converting text to lowercase.\n",
        "- Removing punctuation, special characters, and extra spaces.\n",
        "- Handling contractions (e.g., \"don't\" → \"do not\").\n",
        "- Reducing words to their root form via lemmatization or stemming.\n",
        "\n",
        "Normalization is crucial for consistent and accurate text processing and analysis.\n",
        "\n",
        "`11. What is the primary use of word embeddings in NLP?`\n",
        "\n",
        "Word embeddings are used to represent words in a continuous vector space where similar words have similar representations. They capture semantic and syntactic relationships between words, enabling:\n",
        "\n",
        "- Improved performance in NLP tasks like sentiment analysis, text classification, and translation.\n",
        "- Understanding of word relationships (e.g., \"man\" + \"queen\" ≈ \"woman\" + \"king\").\n",
        "- Reducing high-dimensional sparse text data (like one-hot encoding) into dense, low-dimensional representations for machine learning models.\n",
        "\n",
        "`12. What is an annotator in NLP?`\n",
        "\n",
        "An annotator in NLP is a tool or process that labels text data with metadata, such as:\n",
        "\n",
        "- Part-of-speech tags (e.g., identifying nouns, verbs).\n",
        "- Named entity recognition (e.g., labeling \"New York\" as a location).\n",
        "- Sentiment labels (e.g., positive, neutral, negative).\n",
        "Annotations are critical for creating labeled datasets for supervised learning and for preprocessing tasks in NLP pipelines.\n",
        "\n",
        "`13. What are the key steps in text processing before applying machine learning models?`\n",
        "\n",
        "The key steps include:\n",
        "\n",
        "- Text Cleaning: Remove special characters, punctuation, and unnecessary spaces.\n",
        "- Tokenization: Split text into words or sentences.\n",
        "- Stopword Removal: Exclude common words (e.g., \"the\", \"is\") that don't add value to analysis.\n",
        "- Text Normalization: Convert text to lowercase, handle contractions, and standardize formatting.\n",
        "- Stemming/Lemmatization: Reduce words to their root forms.\n",
        "- Feature Extraction: Use methods like bag-of-words, TF-IDF, or word embeddings to represent text numerically.\n",
        "- Handling Missing Values: Fill or remove text entries with missing data.\n",
        "- Encoding: Convert categorical text data into machine-readable formats.\n",
        "\n",
        "`14. What is the history of NLP and how has it evolved?`\n",
        "\n",
        "1950s–1970s (Early Years):\n",
        "\n",
        "Rule-based systems and symbolic AI.\n",
        "Introduction of the Turing Test by Alan Turing.\n",
        "Development of context-free grammars (Chomsky).\n",
        "\n",
        "1980s–1990s (Statistical NLP):\n",
        "\n",
        "Shift to data-driven approaches with statistical models.\n",
        "Use of n-grams, Hidden Markov Models (HMMs), and decision trees.\n",
        "\n",
        "2000s (Machine Learning Era):\n",
        "\n",
        "Integration of machine learning techniques like SVMs, CRFs, and logistic regression.\n",
        "Emergence of large-scale annotated datasets like WordNet and corpora like Penn Treebank.\n",
        "\n",
        "2010s–Present (Deep Learning Revolution):\n",
        "\n",
        "Introduction of word embeddings (e.g., Word2Vec, GloVe).\n",
        "Success of RNNs, LSTMs, and GRUs for sequence modeling.\n",
        "Rise of transformer-based architectures (e.g., BERT, GPT) for contextual language understanding.\n",
        "\n",
        "`15. Why is sentence processing important in NLP?`\n",
        "\n",
        "Sentence processing is crucial because sentences are the fundamental units of meaning in text. It enables:\n",
        "\n",
        "- Semantic Understanding: Ensures accurate comprehension of context and relationships between words.\n",
        "- Syntax Parsing: Helps in grammar analysis, identifying subjects, verbs, and objects.\n",
        "- Task-Specific Applications: Essential for tasks like machine translation, sentiment analysis, and summarization.\n",
        "- Coherence Maintenance: Ensures logical flow and meaning across sentences in larger texts.\n",
        "\n",
        "`16. How do word embeddings improve the understanding of language semantics in NLP?`\n",
        "\n",
        "Word embeddings enhance semantic understanding by:\n",
        "\n",
        "- Capturing Contextual Meaning: Words with similar meanings are located close in the vector space. For example, \"cat\" and \"kitten\" will have similar embeddings.\n",
        "- Understanding Relationships: Encodes linguistic relationships (e.g., gender, pluralization).\n",
        "- Generalization: Enables models to understand unseen data based on similarities learned during training.\n",
        "- Reducing Dimensionality: Converts high-dimensional sparse data into dense, low-dimensional vectors for efficient computation.\n",
        "\n",
        "These capabilities allow models to perform better in tasks like question answering, text generation, and sentiment analysis.\n",
        "\n",
        "`19. What is the difference between Word2Vec and Doc2Vec?`\n",
        "\n",
        "Word2Vec and Doc2Vec are both algorithms for representing text as vectors, but they differ in focus and application:\n",
        "\n",
        "Word2Vec:\n",
        "\n",
        "- Focuses on individual words.\n",
        "- Produces dense vector representations for words based on their context (e.g., Skip-gram, CBOW).\n",
        "- Captures semantic relationships between words.\n",
        "- Suitable for tasks like synonym detection, word similarity, and analogy solving.\n",
        "\n",
        "Doc2Vec:\n",
        "\n",
        "- Extends Word2Vec to represent entire documents (or sentences, paragraphs).\n",
        "- Introduces an additional document ID vector to learn representations for variable-length texts.\n",
        "- Useful for tasks like document clustering, classification, and sentiment analysis.\n",
        "\n",
        "`20. Why is understanding text normalization important in NLP?`\n",
        "\n",
        "Text normalization ensures consistency in text data, which is crucial for accurate analysis and modeling. Benefits include:\n",
        "\n",
        "- Improved Accuracy: Standardizing text reduces variations (e.g., \"USA\" vs. \"U.S.A.\") and prevents treating similar items as different.\n",
        "- Noise Reduction: Removes unnecessary characters, punctuation, and formatting issues.\n",
        "- Better Tokenization: Simplifies splitting and processing of text into meaningful units.\n",
        "- Enhanced Generalization: Helps models generalize across variations in text (e.g., case-insensitive).\n",
        "\n",
        "`21. How does word count help in text analysis?`\n",
        "\n",
        "Word count is a foundational metric that aids in:\n",
        "\n",
        "- Frequency Analysis: Identifies the most common or significant terms in a text.\n",
        "- Keyword Extraction: Highlights important words for summarization or search optimization.\n",
        "- Content Length Assessment: Measures verbosity or conciseness of text.\n",
        "- Feature Engineering: Forms the basis of bag-of-words (BoW) and term frequency-inverse document frequency (TF-IDF) representations.\n",
        "\n",
        "`22. How does lemmatization help in NLP tasks like search engines and chatbots?`\n",
        "\n",
        "Lemmatization reduces words to their canonical or base form, improving NLP tasks by:\n",
        "\n",
        "- Enhancing Search Relevance: Ensures queries and documents match by considering variations (e.g., \"running\" → \"run\").\n",
        "- Improving Intent Recognition: Reduces complexity for chatbots by standardizing word forms, leading to better intent matching.\n",
        "- Reducing Vocabulary Size: Simplifies text representations, improving model efficiency.\n",
        "\n",
        "`23. What is the purpose of using Doc2Vec in text processing?`\n",
        "\n",
        "Doc2Vec is used to generate dense vector representations for variable-length texts like sentences, paragraphs, or documents. Its purposes include:\n",
        "\n",
        "- Document Classification: Enables efficient categorization of texts.\n",
        "- Semantic Search: Finds similar documents based on content rather than exact matches.\n",
        "- Clustering and Recommendation: Groups similar documents for recommendations.\n",
        "Sentiment Analysis: Encodes text sentiment contextually for analysis.\n",
        "\n",
        "`24. What is the importance of sentence processing in NLP?`\n",
        "\n",
        "Sentence processing is critical because sentences form the basic unit of meaning in text. Importance includes:\n",
        "\n",
        "- Syntax Parsing: Analyzes grammatical structure for subject-verb-object relationships.\n",
        "- Context Preservation: Maintains coherence in tasks like summarization and translation.\n",
        "- Task-Specific Applications: Powers tasks like sentiment analysis, machine translation, and question answering.\n",
        "- Enhanced Understanding: Helps models interpret sentence-level nuances, improving overall NLP performance.\n",
        "\n",
        "\n",
        "`28. What is the primary purpose of text processing in NLP?`\n",
        "\n",
        "Text processing in NLP aims to transform raw text into a structured and analyzable format for further processing or modeling. Key purposes include:\n",
        "\n",
        "- Noise Reduction: Removes unnecessary elements like punctuation, special characters, and stopwords.\n",
        "- Standardization: Ensures uniformity (e.g., lowercasing, lemmatization, stemming).\n",
        "- Feature Extraction: Converts text into numerical representations (e.g., bag-of-words, TF-IDF, word embeddings).\n",
        "- Improved Model Performance: Prepares data for efficient and accurate machine learning or NLP models.\n",
        "\n",
        "`29. What are the key challenges in NLP?`\n",
        "\n",
        "NLP faces several challenges due to the complexity and variability of human language, such as:\n",
        "\n",
        "- Ambiguity: Words or sentences can have multiple meanings depending on context.\n",
        "- Context Understanding: Capturing long-range dependencies or implicit meanings.\n",
        "- Language Diversity: Handling different languages, dialects, and scripts.\n",
        "- Data Sparsity: Insufficient labeled data for certain tasks or languages.\n",
        "- Syntax and Grammar Variability: Diverse sentence structures and informal writing styles.\n",
        "- Bias and Fairness: Avoiding bias in training data that could affect predictions.\n",
        "\n",
        "`30. How do co-occurrence vectors represent relationships between words?`\n",
        "\n",
        "Co-occurrence vectors capture the relationships between words based on their proximity in a text corpus:\n",
        "\n",
        "- Construction: Represent words as vectors where each dimension corresponds to the frequency of co-occurrence with another word in a fixed context window.\n",
        "- Semantic Relationships: Words with similar contexts (e.g., \"king\" and \"queen\") tend to have similar vector representations.\n",
        "- Applications: Used in creating word embeddings, building semantic networks, and measuring word similarity.\n",
        "\n",
        "\n",
        "`31. What is the role of frequency distribution in text analysis?`\n",
        "\n",
        "Frequency distribution is a fundamental tool in text analysis for:\n",
        "\n",
        "- Identifying Common Words: Highlights frequently occurring terms, aiding in keyword extraction or topic identification.\n",
        "- Understanding Text Characteristics: Provides insights into the style, themes, and focus of the text.\n",
        "- Feature Engineering: Forms the basis for bag-of-words and TF-IDF models.\n",
        "- Removing Noise: Helps identify and eliminate stopwords or irrelevant terms.\n",
        "\n",
        "`32. What is the impact of word embeddings on NLP tasks?`\n",
        "\n",
        "Word embeddings have transformed NLP by offering dense, semantic-rich representations of words. Impacts include:\n",
        "\n",
        "- Contextual Understanding: Captures relationships and analogies (e.g., \"man:king :: woman:queen\").\n",
        "- Improved Model Performance: Enables efficient and accurate learning in tasks like classification, translation, and sentiment analysis.\n",
        "- Transfer Learning: Pretrained embeddings like Word2Vec or GloVe can be reused across tasks, reducing the need for extensive labeled data.\n",
        "- Reduced Dimensionality: Compresses high-dimensional sparse data into dense vectors while retaining meaning.\n",
        "\n",
        "`33. What is the purpose of using lemmatization in text preprocessing?`\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form, improving text preprocessing by:\n",
        "\n",
        "- Standardizing Text: Groups inflected forms of a word (e.g., \"running,\" \"ran\" → \"run\").\n",
        "- Reducing Vocabulary Size: Simplifies data for models, enhancing efficiency and reducing overfitting.\n",
        "- Improving Search and Matching: Ensures better matching of queries with documents or user inputs.\n",
        "- Maintaining Meaning: Unlike stemming, lemmatization considers the context and returns valid base forms, preserving meaning."
      ],
      "metadata": {
        "id": "2Un7nOzKOkx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "m251nmLO35En"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>1. How can you perform word tokenization using NLTK?\n"
      ],
      "metadata": {
        "id": "ovPXFNGQ3DUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Natural Language Processing is fascinating.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hniqoQ448tR",
        "outputId": "4b84409c-4141-48e8-9091-6ca08f8b86bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>2. How can you perform sentence tokenization using NLTK?\n"
      ],
      "metadata": {
        "id": "stq8bYJ-3l9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Natural Language Processing is fascinating. It has many applications.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAgDDmxt4qAb",
        "outputId": "3839fa98-6c7e-4764-9b73-6a11b7ad201a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural Language Processing is fascinating.', 'It has many applications.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>3. How can you remove stopwords from a sentence?\n"
      ],
      "metadata": {
        "id": "QNMJoPgQ3pfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a simple sentence to demonstrate removing stopwords.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgVaSysY5Hr8",
        "outputId": "57a7a029-49e9-4573-b9df-9612a400c6a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'sentence', 'demonstrate', 'removing', 'stopwords', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>4. How can you perform stemming on a word?\n"
      ],
      "metadata": {
        "id": "kqySAxTt3sTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "word = \"running\"\n",
        "stemmed_word = stemmer.stem(word)\n",
        "print(stemmed_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXF68QHP5TrO",
        "outputId": "4cc5c140-1a2d-443b-d8bc-c098a5944cd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>5. How can you perform lemmatization on a word?"
      ],
      "metadata": {
        "id": "tcJGYIOm3vBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = \"running\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos=\"v\")\n",
        "print(lemmatized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hjzKxKb5Xzz",
        "outputId": "d4af8826-38c6-43b0-aee9-cdb975ed151c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>6. How can you normalize a text by converting it to lowercase and removing punctuation?\n"
      ],
      "metadata": {
        "id": "UlLnNa4d3wUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text = \"Hello, World! This is Text Normalization.\"\n",
        "normalized_text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcmjG9SQ68WT",
        "outputId": "114fe75c-364c-4193-9b32-824a7b7db1cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world this is text normalization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>7. How can you create a co-occurrence matrix for words in a corpus?\n"
      ],
      "metadata": {
        "id": "02H8nnTF6kyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\"the cat is on the mat\", \"the mat is in the room\"]\n",
        "\n",
        "# Tokenize and prepare co-occurrence data\n",
        "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
        "vocabulary = set(word for sentence in tokenized_corpus for word in sentence)\n",
        "vocab_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "co_occurrence = np.zeros((len(vocabulary), len(vocabulary)))\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    for word1, word2 in combinations(sentence, 2):\n",
        "        i, j = vocab_index[word1], vocab_index[word2]\n",
        "        co_occurrence[i, j] += 1\n",
        "        co_occurrence[j, i] += 1\n",
        "\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence, index=list(vocabulary), columns=list(vocabulary))\n",
        "print(co_occurrence_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l0asEfL7Aes",
        "outputId": "b12b1925-4aaf-42c1-8057-3c4cdf6b34ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      mat  room   is   in   on  the  cat\n",
            "mat   0.0   1.0  2.0  1.0  1.0  4.0  1.0\n",
            "room  1.0   0.0  1.0  1.0  0.0  2.0  0.0\n",
            "is    2.0   1.0  0.0  1.0  1.0  4.0  1.0\n",
            "in    1.0   1.0  1.0  0.0  0.0  2.0  0.0\n",
            "on    1.0   0.0  1.0  0.0  0.0  2.0  1.0\n",
            "the   4.0   2.0  4.0  2.0  2.0  4.0  2.0\n",
            "cat   1.0   0.0  1.0  0.0  1.0  2.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>8. How can you apply a regular expression to extract all email addresses from a text?\n"
      ],
      "metadata": {
        "id": "gHv2SWPn6nU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Contact us at abc@example.com or sales@company.org\"\n",
        "emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F2PdzMb7cO-",
        "outputId": "c13618d6-26ff-4149-de09-8a3e2561ecc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc@example.com', 'sales@company.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>9. How can you perform word embedding using Word2Vec?\n"
      ],
      "metadata": {
        "id": "soip8atw6pwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example sentences\n",
        "sentences = [[\"hello\", \"world\"], [\"machine\", \"learning\", \"is\", \"fun\"]]\n",
        "\n",
        "# Train Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=2)\n",
        "word_vector = model.wv[\"hello\"]\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W93abWJ17o6D",
        "outputId": "affe2b98-9569-4af5-ef35-e8bc91e295e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>10. How can you use Doc2Vec to embed documents?\n"
      ],
      "metadata": {
        "id": "I2X-BNt26sBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Example documents\n",
        "documents = [TaggedDocument(words=[\"hello\", \"world\"], tags=[\"doc1\"]),\n",
        "             TaggedDocument(words=[\"machine\", \"learning\", \"is\", \"fun\"], tags=[\"doc2\"])]\n",
        "\n",
        "# Train Doc2Vec\n",
        "model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=2)\n",
        "doc_vector = model.dv[\"doc1\"]\n",
        "print(doc_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSBMCFBc7s9G",
        "outputId": "0ad028f5-1aff-4ba3-e355-ee0bea1b9519"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-5.2314587e-03 -5.9798616e-03 -9.8819686e-03  8.5538970e-03\n",
            "  3.5665543e-03  2.6306405e-04 -9.8818420e-03 -5.1672836e-03\n",
            " -9.7191567e-03  2.0110265e-03  2.8306588e-03  4.6441266e-03\n",
            " -4.2978036e-03 -3.1460931e-03 -3.0791657e-03 -8.7229870e-03\n",
            "  2.1727502e-03  9.2267562e-03 -9.5030349e-03 -3.4585111e-03\n",
            " -3.7703724e-03  2.6077030e-03 -5.6922561e-03  2.6210023e-03\n",
            "  5.8032344e-03 -8.1078568e-03 -8.3308145e-03 -9.9558933e-03\n",
            "  4.9336511e-03 -9.1234287e-03  5.8426815e-03  6.8010986e-03\n",
            " -6.5071997e-03 -4.5204367e-03 -1.2550156e-03  1.6465231e-03\n",
            " -1.4815197e-03 -8.5435910e-03 -3.6030561e-03  1.7318386e-03\n",
            " -2.0571721e-03 -7.2309305e-03  4.1851141e-03 -8.5753947e-03\n",
            "  2.7118700e-03 -4.6142875e-03  6.4550707e-04 -2.0576001e-03\n",
            "  5.4138936e-03 -8.0035543e-03 -2.1201116e-03 -9.5827432e-05\n",
            " -6.6395933e-03 -6.5269656e-03 -1.9331960e-03  8.8045569e-03\n",
            " -1.2633244e-03  3.5364146e-03 -5.7510198e-03  8.8158976e-03\n",
            "  2.9158266e-03  9.2808260e-03  4.3503898e-03 -4.2000851e-03\n",
            "  2.2421815e-03 -4.4129980e-03  5.7776505e-03  1.8317483e-03\n",
            " -2.2790409e-03 -5.8818413e-03 -8.0280704e-03 -8.5317722e-04\n",
            " -8.9404620e-03 -9.2247678e-03 -7.9408856e-03  2.1693404e-03\n",
            " -6.5017394e-03 -7.7893008e-03  2.1314295e-03  2.0529148e-03\n",
            "  8.3493832e-03  4.6684886e-03 -9.4112605e-03 -3.3882252e-04\n",
            "  7.8549925e-03  2.6718038e-03  2.6806931e-03 -4.8843115e-03\n",
            "  6.4679654e-03  1.6490173e-03 -7.6030786e-03  6.8648160e-03\n",
            " -9.7705983e-03 -8.1595974e-03 -4.8747626e-03  9.9388985e-03\n",
            "  3.1134747e-03 -2.0122842e-03  8.8962633e-03  2.3515455e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>11. How can you perform part-of-speech tagging?\n"
      ],
      "metadata": {
        "id": "VhcQB1iC6uSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"Natural Language Processing is fascinating.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrjPjMfJ7xaE",
        "outputId": "f35fed2e-220c-42a8-99d4-982adbff2b95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>12. How can you find the similarity between two sentences using cosine similarity?"
      ],
      "metadata": {
        "id": "7DjiqPeB6wjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sentence1 = \"Natural Language Processing is fascinating.\"\n",
        "sentence2 = \"NLP is a very interesting field.\"\n",
        "\n",
        "# Vectorize sentences\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "print(similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di8ApGLC6yZv",
        "outputId": "c08ba941-7d6f-44f1-902f-b49a214b303b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11234278]]\n"
          ]
        }
      ]
    }
  ]
}